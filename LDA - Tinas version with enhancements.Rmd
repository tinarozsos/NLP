---
title: "LDA"
author: "Tina, Aleksandr, Mike"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r pacakges, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
library(tm)
library(textclean)
library(stopwords)
library(topicmodels)
library(patchwork)
library(textstem)
library(textmineR)
```


```{r Tokenisation}
# Tokenization ------------------------------------------------------------
# improve with phrases, etc.

library(sjmisc)

questions <- read_csv("data/questions.csv")

stopwords<- c(data_stopwords_snowball$en, 
                      "eu", "european", "commission", "member", 
                      "can", "view", "submitted", "rights", "states", 
                      "state", "ensure" )
words <- questions %>% 
  mutate(text = str_replace_all(text, "\\[\\d+\\]|\n|https?://\\S+", " "),
         text = replace_contraction(text), 
         text = removeNumbers(text),
         text = str_replace_all(text, "\\s+", " "),
         text = lemmatize_strings(text))
  

# Extract unigrams
unigrams <- words %>%
  unnest_tokens(word, text, token = "words") %>%
  filter(!(word %in% stopwords))

# Extract bigrams & remove those containing stopwords 
bigrams <- words %>%
  unnest_tokens(word, text, token = "ngrams", n = 2) %>%
  separate(word, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stopwords, 
         !word2 %in% stopwords) %>%
  unite(word, word1, word2, sep = " ")

  
words <- bind_rows(unigrams, bigrams)


frequent_words <- words %>% 
  count(word) %>% 
  filter(n >= 20) %>%  # Adjust the threshold as needed
  select(word)

words <- words %>%
  filter(word %in% frequent_words$word)

```

```{r LDA}
# Fit LDA -----------------------------------------------------------------
set.seed(123)
# get document-term matrix

proportion = 50

n_topics = 10

width_height = 32

font_size = 2

dtm <- words %>% 
  count(url, word) %>% 
  bind_tf_idf(word, url, n) %>% 
  # keep only words that appear in less than 90% of documents
  slice_max(idf, prop = (proportion/100)) %>%
  cast_dtm(url, word, n)

# LDA with different number of topics
#for (k in 2:10) {
for (k in c(n_topics)) {
  # LKA with k topics
  lda <- LDA(dtm, k = k, control = list(seed = 1))
  
  # word allocation to topics
  word_topics <- tidy(lda, matrix = "beta")
  
  # document allocation to topics
  doc_topics <- tidy(lda, matrix = "gamma") %>% 
    group_by(document) %>% 
    slice_max(gamma, n = 1)
  
  # main words per topic
  p1 <- word_topics %>% 
    mutate(topic = paste("Topic", topic)) %>%
    group_by(topic) %>% 
    slice_max(beta, n = 10) %>% 
    ggplot(aes(beta, reorder(term, beta), fill = topic)) + 
    geom_col() +
    labs(y = "", x = "Per-topic-per-word probability",
         fill = "Topic") +
    scale_x_continuous(n.breaks = 3) +
    facet_wrap(~topic, scales = "free_y") +
    theme_minimal() +
    theme(legend.position = "none") +
    theme(axis.text = element_text(size = font_size))
  
  # topic distribution per party
  p2 <- doc_topics %>% 
    mutate(topic = paste("Topic", topic)) %>%
    left_join(select(questions, url, party), by = c("document" = "url")) %>%
    ggplot(aes(y = party, fill = topic)) +
    geom_bar(position = "fill") +
    scale_x_continuous(labels = scales::percent) +
    labs(y = "Political group", x = "Proportion of questions",
         fill = "Topic") +
    theme_minimal()+
    theme(axis.text = element_text(size = font_size))
  
  # topic distribution per region
  p3 <- doc_topics %>% 
    mutate(topic = paste("Topic", topic)) %>%
    left_join(select(questions, url, region), by = c("document" = "url")) %>%
    ggplot(aes(y = region, fill = topic)) +
    geom_bar(position = "fill") +
    scale_x_continuous(labels = scales::percent) +
    labs(y = "Region", x = "Proportion of questions",
         fill = "Topic") +
    theme_minimal()+
    theme(axis.text = element_text(size = font_size))
  
  p1 / (p2 + p3) + plot_layout(guides = "collect") & theme(legend.position = "bottom")
  ggsave(paste0("results/lda_", k, "proption_", proportion, ".png"), width = width_height, height = width_height)
}
```
